# SignSpace - Real-Time ASL Learning with Apple Vision Pro

![SignSpace Banner](https://img.shields.io/badge/visionOS-2.5-blue) ![Swift](https://img.shields.io/badge/Swift-5.9-orange) ![License](https://img.shields.io/badge/license-MIT-green)

**Learn American Sign Language with instant, personalized feedback using spatial computing.**

Built at USC's **Good Vibes Only Buildathon 2025** | a16z x PayPal x Lovable x META x Apple

## ğŸ¯ The Problem

- **430 million people worldwide** need hearing support and rehabilitation  
- **500,000+ people** in the US use American Sign Language  
- Learning ASL is challenging without real-time, personalized feedback  
- Traditional methods (videos, books) lack interactive guidance  
- No way to verify if you're making signs correctly  

## ğŸ’¡ Our Solution

**SignSpace** leverages Apple Vision Pro's advanced hand tracking to provide:

- **Real-time gesture recognition** â€“ Detects ASL signs using ML-based classification  
- **Specific, actionable feedback** â€“ â€œMove your thumb closer to your palmâ€ instead of generic errors  
- **Progress tracking** â€“ Visual indicators showing mastery of each sign  
- **Spatial visualization** â€“ 3D hand skeleton with joint-level accuracy
- **Ghost hands overlay (Unused for now)** â€“ Transparent â€œtargetâ€ hands show the correct position (will be expanded upon in the next update) 

## ğŸ¥ Demo

> **[Demo Video Link Here](https://drive.google.com/file/d/1XPNRzenzS-k5-pO7UJ3PRrQrqJHE6at7/view?usp=sharing)**

### Key Features Showcased
1. Hand tracking initialization  
2. Learning letter "A" with instant feedback  
3. Real-time corrections and validation  
4. Progress through 5 ASL signs  
5. Celebratory confetti on mastery  

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **visionOS 2.5** â€“ Native Apple Vision Pro development  
- **Swift 5.9** â€“ Modern, type-safe programming  
- **SwiftUI** â€“ Declarative UI framework  
- **RealityKit** â€“ 3D rendering and spatial computing  
- **Hand Tracking API** â€“ Real-time 27-joint precision tracking  
- **Core ML** â€“ Integrated ASL gesture classifier  

### Key Features
- **Rule-Based + ML Gesture Recognition Engine** â€“ Combines CoreML model with rule-based validation  
- **Spatial Hand Visualization** â€“ 3D skeleton rendering with joint connections  
- **Adaptive Feedback System** â€“ Confidence-based color coding (red/yellow/green)  
- **CSV Export + Share Sheet** â€“ Export recorded samples for model retraining  
- **Mock Data Support** â€“ Simulator testing without physical hardware  

## ğŸ—ï¸ Architecture
```
SignSpace/
â”œâ”€â”€ SignSpaceApp.swift # App entry point + immersive space setup
â”œâ”€â”€ AppModel.swift # App state management
â”œâ”€â”€ ContentView.swift # Main learning UI + feedback logic
â”œâ”€â”€ ConfettiView.swift # Confetti animation when sign mastered
â”œâ”€â”€ DataCollectionView.swift # Data collection & CSV export interface
â”œâ”€â”€ CSVExporter.swift # Session data tracking & CSV writer
â”œâ”€â”€ GestureRecognizer.swift # Rule-based ASL gesture recognition
â”œâ”€â”€ MLGestureRecognizer.swift # CoreML-based gesture prediction
â”œâ”€â”€ GhostHandData.swift # Ideal hand joint positions per sign 
â”œâ”€â”€ HandTrackingManager.swift # ARKit session + real-time hand tracking
â”œâ”€â”€ HandTrackingComponent.swift # RealityKit component for hand entities
â”œâ”€â”€ HandTrackingSystem.swift # System managing ARKit anchor updates
â”œâ”€â”€ HandTrackingView.swift # 3D hand entity rendering view
â”œâ”€â”€ ImmersiveView.swift # RealityView for immersive mode
â”œâ”€â”€ SoundManager.swift # Audio feedback for success/errors
â”œâ”€â”€ ToggleImmersiveSpaceButton.swift # Button to toggle immersive view
â””â”€â”€ Assets/ # ASL sign images + app assets
```

### Data Flow
```
Vision Pro Hand Tracking
        â†“
HandTrackingManager (extracts 27 joints)
        â†“
MLGestureRecognizer + GestureRecognizer (classifies + validates)
        â†“
ContentView (shows confidence, feedback, confetti)
        â†“
User sees real-time corrections + progress tracking
```


## ğŸ¨ Features

### 1. **Real-Time Hand Tracking**
- Powered by `ARKitSession` and `HandTrackingProvider`
- 27 tracked joints with live position updates
- Works seamlessly in immersive space

### 2. **Ghost Hands Overlay**
- Shows ideal ASL hand positions (from `GhostHandData`)
- Used for reference and spatial guidance  

### 3. **Dual Recognition System**
- **Rule-based:** geometry & distances between joints  
- **ML-powered:** CoreML model trained from real CSV data  

### 4. **Intelligent Feedback**
- **Green (â‰¥85%)**: Perfect! ğŸ‰  
- **Yellow (65â€“85%)**: Almost there!  
- **Red (<65%)**: Needs correction  
- **Gray**: No hand detected  

### 5. **Progress Tracking & Celebration**
- Tracks completed signs  
- Displays progress bar  
- Confetti on completion  

### 6. **Sound Feedback**
- Success tone (correct)  
- Progress tone (improving)  
- Error tone (incorrect)  

### 7. **Data Collection & Export**
- Record hand samples for each sign  
- Export labeled CSV for training CoreML models  
- Share sheet for direct file sharing  

## ğŸ“š Supported ASL Signs

| Sign | Description | Difficulty |
|------|-------------|-------------|
| **A** | Closed fist, thumb on side | Easy |
| **B** | Fingers straight up, thumb tucked | Medium |
| **C** | Hand forms "C" curve | Easy |
| **Hello** | Open hand, all fingers extended | Easy |
| **Thank You** | Flat hand, fingers together (custom sign for testing) | Medium |

**Future:** Expand to full alphabet and 50+ phrases  

## ğŸš€ Getting Started

### Prerequisites
- macOS 14.0 or later  
- Xcode 15.0 or later  
- Apple Vision Pro (for real testing)  
- Apple Developer account  

### Installation
1. **Install repo**
```bash
git clone https://github.com/yourusername/SignSpace.git
cd SignSpace
open SignSpace.xcodeproj
```

2. **Open in Xcode**
```bash
   open SignSpace.xcodeproj
```

3. **Add Hand Tracking Capability**
   - Select SignSpace project â†’ Target â†’ Signing & Capabilities
   - Click "+ Capability"
   - Add "Hand Tracking"

4. **Build and Run**
   - Select "Apple Vision Pro" simulator or physical device
   - Press `Cmd + R`

### Testing Without Vision Pro

The app includes **mock hand tracking** for simulator testing:
```swift
// In HandTrackingManager.swift (line 26)
var useMockData = true  // Simulator mode with animated hands
```

For real device testing:
```swift
var useMockData = false  // Real Vision Pro hand tracking
```

## ğŸ“ How It Works

### Hand Tracking Pipeline

1. **Initialize Session**
   - Requests `handTracking` authorization from ARKit.
   - Starts `ARKitSession` with `HandTrackingProvider`.
   - Continuously processes anchor updates at 90Hz for both hands.

2. **Extract Joint Data**
   - Captures 27 joints per hand: wrist, thumb, index, middle, ring, and pinky.
   - Converts joint transforms to world coordinates using `originFromAnchorTransform`.
   - Stores as `SIMD3<Float>` for spatial calculations.

3. **Gesture Recognition**
   - Two engines work in parallel:
     - **Rule-based**: Uses distances and angles between joints for precision feedback.
     - **ML-powered**: CoreML model (`ASLClassifierReal1.mlmodel`) predicts gestures from 12 extracted features (6 joints Ã— 2D coordinates).
   - Combines both methods for accurate and interpretable recognition.

4. **Generate Feedback**
   - Calculates a confidence score (0â€“1).
   - Produces contextual feedback:
     - â€œPerfect!â€ if confidence > 0.9  
     - â€œGood try!â€ if 0.6â€“0.9  
     - â€œShow your handâ€ if tracking is lost
   - Feedback color dynamically updates (green/yellow/red/gray).

5. **Render Visualization**
   - `RealityView` displays real-time 3D hand skeletons.
   - `GhostHandData` overlays â€œidealâ€ hand position for the current sign.
   - Visual and audio cues provide instant correction guidance.

6. **Track Progress**
   - `ContentView` maintains user progress via `signsLearned` state.
   - Confetti and success sound trigger on first mastery.
   - Progress indicators fill based on completed signs.

7. **Data Collection Mode**
   - Switch to `DataCollectionView` for ML training.
   - Captures hand features for each ASL sign and saves them as CSV.
   - Built-in `ShareSheet` enables exporting data directly.

## ğŸ† Technical Highlights

### Why This Showcases Vision Pro
- **Native Hand Tracking:** Uses Vision Proâ€™s most advanced capability without external hardware, being privacy-preserving (processed locally).  
- **Spatial Feedback Loop:** Ghost hands are anchored in 3D space, enabling natural alignment.  
- **Real-Time Performance:** 90Hz input rate with <10ms latency feedback pipeline.  
- **On-Device ML:** All gesture processing is performed locally using CoreML for privacy and speed.  
- **Immersive Accessibility-First Learning:** Users learn through kinesthetic feedback rather than static visuals, making ASL learning accessible to all.  

## ğŸ”® Future Roadmap

## ğŸ”® Future Roadmap

### Phase 1 (MVP â€“ Completed)
- [x] Vision Pro hand tracking integration  
- [x] CoreML gesture recognition (5 signs)  
- [x] Visual and audio feedback system  
- [x] Confetti + progress tracking  
- [x] CSV export for model retraining  

### Phase 2 (Next 3 Months)
- [ ] Full ASL alphabet coverage  
- [ ] Improved CoreML accuracy with larger dataset 
- [ ] Lesson-based user flow  
- [ ] Advanced analytics dashboard  

### Phase 3 (6 Months)
- [ ] SharePlay multiplayer mode  
- [ ] Video recording and playback  
- [ ] BSL/LSF language expansion  

### Phase 4 (12 Months)
- [ ] Enterprise integration (schools, hospitals)  
- [ ] Collaboration with accessibility researchers  
- [ ] iOS companion app for progress tracking

## ğŸ¤ Contributing
We welcome contributions! Here's how:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Areas for Contribution
- Adding more ASL signs
- Improving gesture recognition accuracy
- UI/UX enhancements
- Accessibility features
- Documentation improvements

## ğŸ‘¥ Team

**Mustafa Nomair** â€“ Computer Science @ USC  
*Led full product development â€” from design and coding to gesture detection, feedback logic, and Vision Pro app integration alongside Abdelaziz.*

**Abdelaziz Abderhman** â€“ Electrical and Computer Engineering B.S. & Computer Engineering M.S. @ USC  
*Handled Vision Pro hardware setup and sensor integration for real-time hand tracking.*

**Ahmed Ataelfadeel** â€“ Electrical and Computer Engineering @ USC  
*Assisted with hardware connectivity under Abdelazizâ€™s guidance.*

**Hamza Wako** â€“ Computer Science and Business Administration @ USC  
*Served as project manager, coordinating tasks, deadlines, and testing cycles.*

**Ardysatrio Fakhri Haroen** â€“ Computer Science (M.S.) @ USC  
*Mentored the team on ML model structure, app architecture, and performance tuning.*

**Built at**: Good Vibes Only Buildathon 2025  
**Location**: USC Information Sciences Institute, Marina Del Rey 

## ğŸ™ Acknowledgments
- **USC Viterbi School of Engineering** - Venue and support
- **PayPal** - Platinum sponsor
- **Lovable** - Technology partner and credits
- **Meta** - Judge participation
- **Microsoft** - Inclusive Tech Lab inspiration
- **Apple** - Apple Vision Pro access 
Special thanks to all mentors and organizers who made this possible!

## ğŸ“ Contact
**Mustafa Nomair**  
- Email: nomair@usc.edu
- LinkedIn: [View Profile](https://www.linkedin.com/in/mustafa-nomair)
- Project Demo: [Video Link](https://drive.google.com/file/d/1XPNRzenzS-k5-pO7UJ3PRrQrqJHE6at7/view?usp=sharing)

*Making the world more inclusive, one sign at a time.* ğŸ¤Ÿ
