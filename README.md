# SignSpace - Real-Time ASL Tutor with Apple Vision Pro

![SignSpace Banner](https://img.shields.io/badge/visionOS-2.5-blue) ![Swift](https://img.shields.io/badge/Swift-5.9-orange) ![License](https://img.shields.io/badge/license-MIT-green)

**Learn American Sign Language with instant, personalized feedback using spatial computing.**

Built at USC's **Good Vibes Only Buildathon 2025** | a16z x Vercel x PayPal x Lovable x Meta x Apple

## ğŸ¯ The Problem

- **430 million people worldwide** need hearing support and rehabilitation  
- **500,000+ people** in the US use American Sign Language  
- Learning ASL is challenging without real-time, personalized feedback  
- Traditional methods (videos, books) lack interactive guidance  
- No way to verify if you're making signs correctly 

## ğŸ’¡ Our Solution

**SignSpace** leverages Apple Vision Pro's advanced hand tracking to provide:

- **Real-time gesture recognition** â€“ Detects ASL signs using ML-based classification  
- **Specific, actionable feedback** â€“ â€œMove your thumb closer to your palmâ€ instead of generic errors  
- **Progress tracking** â€“ Visual indicators showing mastery of each sign  
- **Spatial visualization** â€“ 3D hand skeleton with joint-level accuracy
- **Ghost hands overlay (Unused for now)** â€“ Transparent â€œtargetâ€ hands show the correct position (will be expanded upon in the next update) 

## ğŸ¥ Demo

> ğŸ¥ **[Demo Video](https://drive.google.com/file/d/1XPNRzenzS-k5-pO7UJ3PRrQrqJHE6at7/view?usp=sharing)**  
> ğŸ“º **[Full Version (if Demo doesnâ€™t load)](https://drive.google.com/file/d/1C_VZJxxrgt15Y8MPfRmrAKQ1d2wWfaRr/view?usp=sharing)**  

**Note:** In the full-length video, youâ€™ll notice that the gesture recognition model was trained on specific hand placements (around 100 samples per sign). Because of that, the system only recognized gestures when they closely matched the positions seen during training. Additionally, the current ML recognizer occasionally references confidence values from other signs when generating feedback -- e.g., it might detect â€œAâ€ and suggest â€œTry making C.â€ Future updates will align the ML feedback system with the same specificity and accuracy as the rule-based mock data version (shown below).


### ğŸ“¸ Vision Pro In-Action

Below are screenshots from **SignSpace** running on Apple Vision Pro, showcasing live hand tracking, sign detection, and interactive feedback.

<p align="center">
  <img src="./images/VisionProA.png" width="75%" alt="SignSpace detecting Sign A"/>
  <br/>
  <em><strong>Sign A Detection:</strong> The system recognizes the sign â€œAâ€ and provides a visual feedback bar indicating accuracy.</em>
</p>

<p align="center">
  <img src="./images/VisonProB.png" width="75%" alt="SignSpace detecting Sign B"/>
  <br/>
  <em><strong>Sign B Detection:</strong> The user performs the sign â€œB,â€ with real-time 3D gesture overlay and feedback display.</em>
</p>

<p align="center">
  <img src="./images/MockDataASL.png" width="75%" alt="SignSpace detecting the sign Hello"/>
  <br/>
  <em><strong>Sign Detection Mode:</strong> The app detects the sign â€œHelloâ€ using Apple Vision Proâ€™s 3D hand tracking and provides real-time, specific feedback (â€œExtend your index finger straight upâ€). Currently, this detailed feedback is available only in the mock data view, with live integration in progress.</em>
</p>

<p align="center">
  <img src="./images/RecordingDataASL.png" width="75%" alt="SignSpace recording gesture samples"/>
  <br/>
  <em><strong>Data Recording Mode:</strong> Used to capture 100 samples per gesture for training the ML model, with clear on-screen instructions for proper hand positioning and motion.</em>
</p>

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **visionOS 2.5** â€“ Native Apple Vision Pro development  
- **Swift 5.9** â€“ Modern, type-safe programming  
- **SwiftUI** â€“ Declarative UI framework  
- **RealityKit** â€“ 3D rendering and spatial computing  
- **Hand Tracking API** â€“ Real-time 27-joint precision tracking  
- **Core ML** â€“ Integrated ASL gesture classifier  

### Key Features
- **Rule-Based + ML Gesture Recognition Engine** â€“ Combines CoreML model with rule-based validation  
- **Spatial Hand Visualization** â€“ 3D skeleton rendering with joint connections  
- **Adaptive Feedback System** â€“ Confidence-based color coding (red/yellow/green)  
- **CSV Export + Share Sheet** â€“ Export recorded samples for model retraining  
- **Mock Data Support** â€“ Simulator testing without physical hardware  

## ğŸ—ï¸ Architecture

The codebase follows an **MVVM (Model-View-ViewModel)** architecture for clean separation of concerns:

```
SignSpace/
â”œâ”€â”€ App/                          # Application layer
â”‚   â”œâ”€â”€ SignSpaceApp.swift        # App entry point + environment injection
â”‚   â””â”€â”€ AppModel.swift            # Immersive space state management
â”‚
â”œâ”€â”€ Views/                        # UI layer (SwiftUI)
â”‚   â”œâ”€â”€ ContentView.swift         # Main learning interface
â”‚   â”œâ”€â”€ DataCollectionView.swift  # Data collection & CSV export UI
â”‚   â”œâ”€â”€ HandTrackingView.swift    # 3D hand entity rendering view
â”‚   â”œâ”€â”€ ImmersiveView.swift       # RealityView for immersive mode
â”‚   â”œâ”€â”€ ConfettiView.swift        # Confetti celebration animation
â”‚   â””â”€â”€ ToggleImmersiveSpaceButton.swift
â”‚
â”œâ”€â”€ ViewModels/                   # Business logic & state
â”‚   â”œâ”€â”€ ContentViewModel.swift    # Learning flow logic + feedback state
â”‚   â””â”€â”€ DataCollectionViewModel.swift
â”‚
â”œâ”€â”€ Services/                     # Core functionality
â”‚   â”œâ”€â”€ HandTrackingManager.swift      # ARKit session + 27-joint tracking
â”‚   â”œâ”€â”€ HybridGestureRecognizer.swift  # Combines ML + rule-based recognition
â”‚   â”œâ”€â”€ MLGestureRecognizer.swift      # CoreML-based gesture prediction
â”‚   â”œâ”€â”€ GestureRecognizer.swift        # Rule-based geometric validation
â”‚   â”œâ”€â”€ HandTrackingSystem.swift       # RealityKit ECS system
â”‚   â”œâ”€â”€ HandTrackingComponent.swift    # RealityKit component
â”‚   â”œâ”€â”€ CSVExporter.swift              # Training data export
â”‚   â””â”€â”€ SoundManager.swift             # Audio feedback
â”‚
â”œâ”€â”€ Models/                       # Data structures
â”‚   â”œâ”€â”€ ASLSign.swift             # Sign enumeration (A, B, C, Hello, ThankYou)
â”‚   â”œâ”€â”€ HandModels.swift          # HandData, HandJoint structs
â”‚   â”œâ”€â”€ GestureResult.swift       # Recognition result container
â”‚   â”œâ”€â”€ GhostHandData.swift       # Ideal hand positions per sign
â”‚   â”œâ”€â”€ TrainingSample.swift      # ML training data structure
â”‚   â””â”€â”€ ASLClassifierReal1.mlmodel # CoreML gesture classifier
â”‚
â””â”€â”€ Assets/                       # ASL sign images + app assets
```

### Data Flow
```
Vision Pro Hand Tracking (27 joints @ 90Hz)
        â†“
HandTrackingManager (ARKit session â†’ HandData)
        â†“
ContentViewModel (100ms polling timer)
        â†“
HybridGestureRecognizer
   â”œâ”€â”€ MLGestureRecognizer (primary, confidence > 0.88)
   â””â”€â”€ GestureRecognizer (rule-based fallback)
        â†“
GestureResult (sign, confidence, feedback)
        â†“
ContentViewModel updates state
        â†“
ContentView renders UI + triggers audio/confetti
        â†“
User sees real-time corrections + progress tracking
```


## ğŸ¨ Features

### 1. **Real-Time Hand Tracking**
- Powered by `ARKitSession` and `HandTrackingProvider`
- 27 tracked joints with live position updates
- Works seamlessly in immersive space

### 2. **Ghost Hands Overlay (unused for now)**
- Shows ideal ASL hand positions (from `GhostHandData`)
- Used for reference and spatial guidance  

### 3. **Dual Recognition System**
- **Rule-based:** geometry & distances between joints  
- **ML-powered:** CoreML model trained from real CSV data  

### 4. **Intelligent Feedback**
- **Green (â‰¥85%)**: Perfect! ğŸ‰  
- **Yellow (65â€“85%)**: Almost there!  
- **Red (<65%)**: Needs correction  
- **Gray**: No hand detected  

### 5. **Progress Tracking & Celebration**
- Tracks completed signs  
- Displays progress bar  
- Confetti on completion  

### 6. **Sound Feedback**
- Success tone (correct)  
- Progress tone (improving)  
- Error tone (incorrect)  

### 7. **Data Collection & Export**
- Record hand samples for each sign  
- Export labeled CSV for training CoreML models  
- Share sheet for direct file sharing  

## ğŸ“š Supported ASL Signs

| Sign | Description | Difficulty |
|------|-------------|-------------|
| **A** | Closed fist, thumb on side | Easy |
| **B** | Fingers straight up, thumb tucked | Medium |
| **C** | Hand forms "C" curve | Easy |
| **Hello** | Open hand, all fingers extended | Easy |
| **Thank You** | Flat hand, fingers together (custom sign for testing) | Medium |

**Future:** Expand to full alphabet and 50+ phrases  

## ğŸš€ Getting Started

### Prerequisites
- macOS 14.0 or later  
- Xcode 15.0 or later  
- Apple Vision Pro (for real testing)  
- Apple Developer account  

### Installation
1. **Install repo**
```bash
git clone https://github.com/yourusername/SignSpace.git
cd SignSpace
open SignSpace.xcodeproj
```

2. **Add Hand Tracking Capability**
   - Select SignSpace project â†’ Target â†’ Signing & Capabilities
   - Click "+ Capability"
   - Add "Hand Tracking"

3. **Build and Run**
   - Select "Apple Vision Pro" simulator or physical device
   - Press `Cmd + R`

### Testing Without Vision Pro

The app includes **mock hand tracking** for simulator testing:
```swift
// In HandTrackingManager.swift (line 17)
var useMockData = true  // Simulator mode with animated hands
```

For real device testing:
```swift
var useMockData = false  // Real Vision Pro hand tracking
```

## ğŸ“ How It Works

### Hand Tracking Pipeline

1. **Initialize Session**
   - Requests `handTracking` authorization from ARKit.
   - Starts `ARKitSession` with `HandTrackingProvider`.
   - Continuously processes anchor updates at 90Hz for both hands.

2. **Extract Joint Data**
   - Captures 27 joints per hand: wrist, thumb, index, middle, ring, and pinky.
   - Converts joint transforms to world coordinates using `originFromAnchorTransform`.
   - Stores as `SIMD3<Float>` for spatial calculations.

3. **Gesture Recognition**
   - `HybridGestureRecognizer` intelligently combines two recognition methods:
     - **ML-powered (primary)**: CoreML model (`ASLClassifierReal1.mlmodel`) predicts gestures from 12 extracted features (6 joints Ã— 2D coordinates). Used when confidence > 0.88.
     - **Rule-based (fallback)**: Uses distances and angles between joints for precision feedback when ML confidence is low.
   - This hybrid approach ensures accurate and interpretable recognition.

4. **Generate Feedback**
   - Calculates a confidence score (0â€“1).
   - Produces contextual feedback:
     - â€œPerfect!â€ if confidence > 0.9  
     - â€œGood try!â€ if 0.6â€“0.9  
     - â€œShow your handâ€ if tracking is lost
   - Feedback color dynamically updates (green/yellow/red/gray).

5. **Render Visualization**
   - `RealityView` displays real-time 3D hand skeletons.
   - `GhostHandData` overlays â€œidealâ€ hand position for the current sign.
   - Visual and audio cues provide instant correction guidance.

6. **Track Progress**
   - `ContentViewModel` maintains user progress via `signsLearned` state.
   - Confetti and success sound trigger on first mastery of each sign.
   - Progress indicators fill based on completed signs.

7. **Data Collection Mode**
   - Switch to `DataCollectionView` for ML training.
   - Captures hand features for each ASL sign and saves them as CSV.
   - Built-in `ShareSheet` enables exporting data directly.

## ğŸ† Technical Highlights

### Why This Showcases Vision Pro
- **Native Hand Tracking:** Uses Vision Proâ€™s most advanced capability without external hardware, being privacy-preserving (processed locally).  
- **Spatial Feedback Loop:** Ghost hands are anchored in 3D space, enabling natural alignment.  
- **Real-Time Performance:** 90Hz input rate with <10ms latency feedback pipeline.  
- **On-Device ML:** All gesture processing is performed locally using CoreML for privacy and speed.  
- **Immersive Accessibility-First Learning:** Users learn through kinesthetic feedback rather than static visuals, making ASL learning accessible to all.  

## ğŸ”® Future Roadmap

### Phase 1 (MVP â€“ Completed)
- [x] Vision Pro hand tracking integration  
- [x] CoreML gesture recognition (5 signs)  
- [x] Visual and audio feedback system  
- [x] Confetti + progress tracking  
- [x] CSV export for model retraining  

### Phase 2 (Next 3 Months)
- [ ] Full ASL alphabet coverage  
- [ ] Improved CoreML accuracy with larger dataset 
- [ ] Lesson-based user flow  
- [ ] Advanced analytics dashboard  

### Phase 3 (6 Months)
- [ ] SharePlay multiplayer mode  
- [ ] Video recording and playback  
- [ ] BSL/LSF language expansion  

### Phase 4 (12 Months)
- [ ] Enterprise integration (schools, hospitals)  
- [ ] Collaboration with accessibility researchers  
- [ ] iOS companion app for progress tracking

## ğŸ‘¥ Team

**Mustafa Nomair** â€“ Computer Science @ USC  
*Led full product development â€” from design and coding to gesture detection, feedback logic, and Vision Pro app integration alongside Abdelaziz.*

**Abdelaziz Abderhman** â€“ Electrical and Computer Engineering B.S. & Computer Engineering M.S. @ USC  
*Handled Vision Pro hardware setup and sensor integration for real-time hand tracking.*

**Hamza Wako** â€“ Computer Science and Business Administration @ USC  
*Served as project manager, coordinating tasks, deadlines, and testing cycles.*

**Built at**: Good Vibes Only Buildathon 2025  
**Location**: USC Information Sciences Institute, Marina Del Rey 

## ğŸ™ Acknowledgments
- **USC Viterbi School of Engineering** - Venue and support
- **Apple** - Apple Vision Pro access 
- **PayPal** - Platinum sponsor
- **Lovable** - Technology partner and credits
- **Meta** - Judge participation
- **Microsoft** - Inclusive Tech Lab inspiration
Special thanks to all mentors and organizers who made this possible!

## ğŸ“ Contact
**Mustafa Nomair**  
- Email: nomair@usc.edu
- LinkedIn: [View Profile](https://www.linkedin.com/in/mustafa-nomair)
- Project Demo: [Video Link](https://drive.google.com/file/d/1XPNRzenzS-k5-pO7UJ3PRrQrqJHE6at7/view?usp=sharing)

## â­ Support
If you enjoyed this project or found it useful, consider giving it a â­ on GitHub - it helps others discover SignSpace!

*Making the world more inclusive, one sign at a time.* ğŸ¤Ÿ
